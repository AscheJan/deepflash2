{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Callbacks\n",
    "\n",
    "> Implements utilities called at certain points during model training in addition to the official tensorflow callbacks (https://www.tensorflow.org/api_docs/python/tf/keras/callbacks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cyclical learning rate policy (CLR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class CyclicLR(tf.keras.callbacks.Callback):\n",
    "    \"\"\"This callback implements a cyclical learning rate policy (CLR).\n",
    "    The method cycles the learning rate between two boundaries with\n",
    "    some constant frequency, as detailed in this paper (https://arxiv.org/abs/1506.01186).\n",
    "    The amplitude of the cycle can be scaled on a per-iteration or\n",
    "    per-cycle basis.\n",
    "    This class has three built-in policies, as put forth in the paper.\n",
    "    \"triangular\":\n",
    "        A basic triangular cycle w/ no amplitude scaling.\n",
    "    \"triangular2\":\n",
    "        A basic triangular cycle that scales initial amplitude by half each cycle.\n",
    "    \"exp_range\":\n",
    "        A cycle that scales initial amplitude by gamma**(cycle iterations) at each\n",
    "        cycle iteration.\n",
    "    For more detail, please see paper.\n",
    "\n",
    "    # Arguments\n",
    "        base_lr: initial learning rate which is the\n",
    "            lower boundary in the cycle.\n",
    "        max_lr: upper boundary in the cycle. Functionally,\n",
    "            it defines the cycle amplitude (max_lr - base_lr).\n",
    "            The lr at any cycle is the sum of base_lr\n",
    "            and some scaling of the amplitude; therefore\n",
    "            max_lr may not actually be reached depending on\n",
    "            scaling function.\n",
    "        step_size: number of training iterations per\n",
    "            half cycle. Authors suggest setting step_size\n",
    "            2-8 x training iterations in epoch.\n",
    "        mode: one of {triangular, triangular2, exp_range}.\n",
    "            Default 'triangular'.\n",
    "            Values correspond to policies detailed above.\n",
    "            If scale_fn is not None, this argument is ignored.\n",
    "        gamma: constant in 'exp_range' scaling function:\n",
    "            gamma**(cycle iterations)\n",
    "        scale_fn: Custom scaling policy defined by a single\n",
    "            argument lambda function, where\n",
    "            0 <= scale_fn(x) <= 1 for all x >= 0.\n",
    "            mode paramater is ignored\n",
    "        scale_mode: {'cycle', 'iterations'}.\n",
    "            Defines whether scale_fn is evaluated on\n",
    "            cycle number or cycle iterations (training\n",
    "            iterations since start of cycle). Default is 'cycle'.\n",
    "        max_momentum(float):     maximum momentum when momentum is cycled\n",
    "                                 If both max_momentum and min_momentum is None,\n",
    "                                 default momentum for Adam is used.\n",
    "                                 (only used if optimizer is Adam)\n",
    "        min_momentum(float):     minimum momentum when momentum is cycled\n",
    "                                 If both max_momentum and min_momentum is None,\n",
    "                                 default momentum for Adam is used.\n",
    "                                 (only used if optimizer is Adam)\n",
    "\n",
    "    References:\n",
    "        Original Paper: https://arxiv.org/abs/1803.09820\n",
    "        Blog Post: https://sgugger.github.io/the-1cycle-policy.html\n",
    "        Code Reference: https://github.com/bckenstler/CLR and https://github.com/amaiya/ktrain\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, base_lr=0.001, max_lr=0.006, step_size=2000., mode='triangular',\n",
    "                 gamma=1., scale_fn=None, scale_mode='cycle',\n",
    "                 reduce_on_plateau=0, monitor='val_loss', reduce_factor=2,\n",
    "                 max_momentum=0.95, min_momentum=0.85, verbose=1):\n",
    "        super(tf.keras.callbacks.Callback, self).__init__()\n",
    "\n",
    "        self.base_lr = base_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.step_size = step_size\n",
    "        self.mode = mode\n",
    "        self.gamma = gamma\n",
    "        if scale_fn == None:\n",
    "            if self.mode == 'triangular':\n",
    "                self.scale_fn = lambda x: 1.\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'triangular2':\n",
    "                self.scale_fn = lambda x: 1/(2.**(x-1))\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'exp_range':\n",
    "                self.scale_fn = lambda x: gamma**(x)\n",
    "                self.scale_mode = 'iterations'\n",
    "        else:\n",
    "            self.scale_fn = scale_fn\n",
    "            self.scale_mode = scale_mode\n",
    "        self.clr_iterations = 0.\n",
    "        self.trn_iterations = 0.\n",
    "        self.history = {}\n",
    "\n",
    "\n",
    "        # annihalting LR\n",
    "        self.overhump = False\n",
    "\n",
    "        # cyclical momentum\n",
    "        self.max_momentum = max_momentum\n",
    "        self.min_momentum = min_momentum\n",
    "        if self.min_momentum is None and self.max_momentum:\n",
    "            self.min_momentum = self.max_momentum\n",
    "        elif self.min_momentum and self.max_momentum is None:\n",
    "            self.max_momentum = self.min_momentum\n",
    "        self.cycle_momentum = True if self.max_momentum is not None else False\n",
    "\n",
    "        self._reset()\n",
    "\n",
    "\n",
    "    def _reset(self, new_base_lr=None, new_max_lr=None,\n",
    "               new_step_size=None):\n",
    "        \"\"\"Resets cycle iterations.\n",
    "        Optional boundary/step size adjustment.\n",
    "        \"\"\"\n",
    "        if new_base_lr != None:\n",
    "            self.base_lr = new_base_lr\n",
    "        if new_max_lr != None:\n",
    "            self.max_lr = new_max_lr\n",
    "        if new_step_size != None:\n",
    "            self.step_size = new_step_size\n",
    "        self.clr_iterations = 0.\n",
    "\n",
    "    def clr(self):\n",
    "        cycle = np.floor(1+self.clr_iterations/(2*self.step_size))\n",
    "        x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)\n",
    "        if self.scale_mode == 'cycle':\n",
    "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)\n",
    "        else:\n",
    "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        logs = logs or {}\n",
    "\n",
    "        if self.clr_iterations == 0:\n",
    "            tf.keras.backend.set_value(self.model.optimizer.lr, self.base_lr)\n",
    "        else:\n",
    "            tf.keras.backend.set_value(self.model.optimizer.lr, self.clr())\n",
    "\n",
    "        self.orig_base_lr = self.base_lr\n",
    "\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "\n",
    "        logs = logs or {}\n",
    "        self.trn_iterations += 1\n",
    "        self.clr_iterations += 1\n",
    "\n",
    "        self.history.setdefault('lr', []).append(tf.keras.backend.get_value(self.model.optimizer.lr))\n",
    "        self.history.setdefault('iterations', []).append(self.trn_iterations)\n",
    "\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "\n",
    "        tf.keras.backend.set_value(self.model.optimizer.lr, self.clr())\n",
    "\n",
    "        # annihilate learning rate\n",
    "        prev_overhump = self.overhump\n",
    "        iterations = (self.clr_iterations+1) % (self.step_size*2)\n",
    "        if iterations/self.step_size > 1:\n",
    "            self.overhump = True\n",
    "        else:\n",
    "            self.overhump = False\n",
    "        if not prev_overhump and self.overhump:\n",
    "            self.base_lr = self.max_lr/1000\n",
    "        elif prev_overhump and not self.overhump:\n",
    "            self.base_lr = self.orig_base_lr\n",
    "\n",
    "        # set momentum\n",
    "        if self.cycle_momentum:\n",
    "            if self.overhump:\n",
    "                current_percentage = 1. - ((iterations - self.step_size) / float(\n",
    "                                            self.step_size))\n",
    "                new_momentum = self.max_momentum - current_percentage * (\n",
    "                    self.max_momentum - self.min_momentum)\n",
    "            else:\n",
    "                current_percentage = iterations / float(self.step_size)\n",
    "                new_momentum = self.max_momentum - current_percentage * (\n",
    "                    self.max_momentum - self.min_momentum)\n",
    "            tf.keras.backend.set_value(self.model.optimizer.beta_1, new_momentum)\n",
    "            self.history.setdefault('momentum', []).append(tf.keras.backend.get_value(self.model.optimizer.beta_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clr = CyclicLR(base_lr=0.001, \n",
    "               max_lr=0.006,\n",
    "               step_size=2000.,\n",
    "               mode='triangular')\n",
    "\n",
    "model.fit(X_train, Y_train, callbacks=[clr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class also supports custom scaling functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clr_fn = lambda x: 0.5*(1+np.sin(x*np.pi/2.))\n",
    "\n",
    "clr = CyclicLR(base_lr=0.001, \n",
    "               max_lr=0.006,\n",
    "               step_size=2000., \n",
    "               scale_fn=clr_fn,\n",
    "               scale_mode='cycle')\n",
    "\n",
    "model.fit(X_train, Y_train, callbacks=[clr])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
