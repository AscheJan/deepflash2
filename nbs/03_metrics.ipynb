{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics\n",
    "\n",
    "> Definition of the metrics that can be used in training models in addition to the official [tensorflow metrics](https://www.tensorflow.org/api_docs/python/tf/keras)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import math_ops, confusion_matrix, array_ops, init_ops\n",
    "from tensorflow.python.framework import dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intersection-Over-Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class IoU(tf.keras.metrics.MeanIoU):\n",
    "    \"\"\"\n",
    "    Computes the Intersection-Over-Union metric.\n",
    "    Adjusted for probabilistic labels and different semantic classes from \n",
    "    tf.keras.metrics.MeanIoU\n",
    "    (https://www.tensorflow.org/api_docs/python/tf/keras/metrics/MeanIoU)\n",
    "    \n",
    "    Mean Intersection-Over-Union is a common evaluation metric for semantic image\n",
    "    segmentation, which first computes the IOU for each semantic class and then\n",
    "    computes the average over classes. IOU is defined as follows:\n",
    "    IOU = true_positive / (true_positive + false_positive + false_negative).\n",
    "    The predictions are accumulated in a confusion matrix, weighted by\n",
    "    `sample_weight` and the metric is then calculated from it.\n",
    "    \n",
    "    If `class_id` is specified, we calculate the IoU by considering only the\n",
    "    entries in the batch for which `class_id` is in the label \n",
    "     \"\"\"\n",
    "    def __init__(self, \n",
    "                 num_classes,\n",
    "                 class_id = None,\n",
    "                 name=None, \n",
    "                 dtype=None):\n",
    "        super().__init__(num_classes=num_classes, name=name, dtype=dtype)\n",
    "        self.class_id = class_id   \n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        \"\"\"Accumulates the confusion matrix statistics.\n",
    "        Args:\n",
    "          y_true: The ground truth values.\n",
    "          y_pred: The predicted values.\n",
    "          sample_weight: Optional weighting of each example. Defaults to 1. Can be a\n",
    "            `Tensor` whose rank is either 0, or the same rank as `y_true`, and must\n",
    "            be broadcastable to `y_true`.\n",
    "        Returns:\n",
    "          Update op.\n",
    "        \"\"\"\n",
    "        if self.class_id is not None:\n",
    "            y_true = y_true[..., self.class_id]\n",
    "            y_pred = y_pred[..., self.class_id]\n",
    "\n",
    "        y_pred = tf.math.round(y_pred)\n",
    "        y_true = math_ops.cast(y_true, self._dtype)\n",
    "        y_pred = math_ops.cast(y_pred, self._dtype)\n",
    "\n",
    "        # Flatten the input if its rank > 1.\n",
    "        if y_pred.shape.ndims > 1:\n",
    "              y_pred = array_ops.reshape(y_pred, [-1])\n",
    "\n",
    "        if y_true.shape.ndims > 1:\n",
    "              y_true = array_ops.reshape(y_true, [-1])\n",
    "\n",
    "        if sample_weight is not None and sample_weight.shape.ndims > 1:\n",
    "              sample_weight = array_ops.reshape(sample_weight, [-1])\n",
    "\n",
    "        # Accumulate the prediction to current confusion matrix.\n",
    "        current_cm = confusion_matrix.confusion_matrix(\n",
    "            y_true,\n",
    "            y_pred,\n",
    "            self.num_classes,\n",
    "            weights=sample_weight,\n",
    "            dtype=dtypes.float64)\n",
    "        return self.total_cm.assign_add(current_cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=864, shape=(2, 2), dtype=float64, numpy=\n",
       "array([[1., 1.],\n",
       "       [0., 2.]])>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = IoU(num_classes=2)\n",
    "m.update_state([0, 0, 1, 1], [0.8, 0.1, 0.99, 0.56])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.result().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__tf.keras.metrics.MeanIoU shows a different behaviour__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(2, 2) dtype=float64, numpy=\n",
       "array([[2., 0.],\n",
       "       [2., 0.]])>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = tf.keras.metrics.MeanIoU(num_classes=2)\n",
    "m.update_state([0, 0, 1, 1], [0.4, 0.1, 0.99, 0.56])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.result().numpy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
