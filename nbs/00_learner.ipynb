{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "#default_exp learner\n",
    "from nbdev.showdoc import show_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import shutil\n",
    "from fastai.vision.all import *\n",
    "from fastcore.all import *\n",
    "from scipy.stats import entropy\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from deepflash2.metrics import Dice_f1, Iou\n",
    "from deepflash2.losses import WeightedSoftmaxCrossEntropy\n",
    "from deepflash2.callbacks import ElasticDeformCallback\n",
    "from deepflash2.models import get_default_shapes\n",
    "from deepflash2.data import TileDataset, RandomTileDataset\n",
    "from deepflash2.utils import iou, plot_results\n",
    "import deepflash2.tta as tta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Patches for the `fastai` Learner\n",
    "\n",
    "> Imlements functions necessary to build `Learner` suitable for bioimgage segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def apply_dropout(self:Learner):\n",
    "    \"If a module contains 'dropout', it will be switched to .train() mode.\"\n",
    "    for m in self.model.modules():\n",
    "        if isinstance(m, nn.Dropout):\n",
    "            m.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def energy_max(e, ks=20, dim=None):\n",
    "    e = torch.as_tensor(e).resize_((1,1,*e.shape))\n",
    "    e = F.avg_pool2d(e, ks)\n",
    "    return torch.max(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = np.random.randn(1024,1024)\n",
    "test_close(energy_max(e, ks=100),0, eps=1e-01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def predict_tiles(self:Learner, ds_idx=1, dl=None, mc_dropout=False, n_times=1, use_tta=False, \n",
    "                  tta_merge='mean', energy_T=1, energy_ks=20, padding=(0,0,0,0)): #(-52,-52,-52,-52)\n",
    "    \"Make predictions and reconstruct tiles, optional with dropout and/or tta applied.\"\n",
    "\n",
    "    if dl is None: dl = self.dls[ds_idx].new(shuffled=False, drop_last=False)\n",
    "    if use_tta: tfms=[tta.HorizontalFlip(), tta.Rotate90(angles=[90,180,270])]\n",
    "    else: tfms=[]\n",
    "\n",
    "    self.model.eval()\n",
    "    if mc_dropout: self.apply_dropout()\n",
    "\n",
    "    smx_means, energy_means, stds = [], [], []\n",
    "    for data in progress_bar(dl, leave=False):\n",
    "        if isinstance(data, TensorImage): images = data\n",
    "        else: images, _, _ = data\n",
    "        m_smx = tta.Merger()\n",
    "        m_energy = tta.Merger()\n",
    "        out_list_smx = []\n",
    "        for t in tta.Compose(tfms):\n",
    "            for _ in range(n_times):\n",
    "                aug_images = t.augment_image(images)\n",
    "                with torch.no_grad():\n",
    "                    out = self.model(aug_images)\n",
    "                out = t.deaugment_mask(out)\n",
    "                if sum(padding)>0: out = F.pad(out, padding)                \n",
    "                m_smx.append(F.softmax(out, dim=1))\n",
    "                e = (energy_T*torch.logsumexp(out/energy_T, dim=1)) #negative energy score\n",
    "                m_energy.append(e)\n",
    "        \n",
    "        stds.append(m_smx.result('std'))\n",
    "        smx_means.append(m_smx.result()) \n",
    "        energy_means.append(m_energy.result()) \n",
    "    \n",
    "    softmax_pred = torch.cat(smx_means).permute(0,2,3,1)\n",
    "    smx_tiles = [x for x in softmax_pred.cpu().numpy()]\n",
    "\n",
    "    std_pred = torch.cat(stds).permute(0,2,3,1)\n",
    "    std_tiles = [x[...,0] for x in std_pred.cpu().numpy()]\n",
    "    \n",
    "    energy_pred = torch.cat(energy_means)#.permute(0,2,3,1)\n",
    "    energy_tiles = [x for x in energy_pred.cpu().numpy()]\n",
    "\n",
    "    smxcores = dl.reconstruct_from_tiles(smx_tiles)\n",
    "    segmentations = [np.argmax(x, axis=-1) for x in smxcores]\n",
    "    std_deviations = dl.reconstruct_from_tiles(std_tiles)\n",
    "    energy_scores = dl.reconstruct_from_tiles(energy_tiles)\n",
    "    \n",
    "    if energy_ks is not None:\n",
    "        energy_scores = [energy_max(e, energy_ks) for e in energy_scores]\n",
    "\n",
    "    return smxcores, segmentations, std_deviations, energy_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def save_tmp(pth_tmp, files, results):\n",
    "    smxs, segs, stds, enrgys = results\n",
    "    pth_tmp.mkdir(exist_ok=True, parents=True)\n",
    "    for i, f in progress_bar(enumerate(files), total=len(files), leave=False):\n",
    "        np.savez(pth_tmp/f'{f.stem}.npz', smx=smxs[i], seg=segs[i], std=stds[i], enrgy=enrgys[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class EnsembleLearner:\n",
    "    def __init__(self, files, label_fn=None, n=5, n_splits=5, pretrained=None, wd=0.001,\n",
    "                 repo='matjesg/deepflash2', arch=\"unext50_deepflash2\", random_state=42,\n",
    "                 metrics=None, loss_fn=None, cbs=None, mpt=False, ds_kwargs={}, \n",
    "                 opt_func=ranger, path=None, ensemble_dir='ensemble', mw_kwargs={},\n",
    "                 stats=None, item_tfms=[Brightness(max_lighting=0.1)]):\n",
    "        store_attr(but='path,ensemble_dir,ds_kwargs,metrics,loss,cbs')\n",
    "        self.path = Path(path) if path is not None else Path('.')\n",
    "        self.ensemble_dir = self.path/ensemble_dir\n",
    "        self.loss_fn = loss_fn or WeightedSoftmaxCrossEntropy(axis=1)\n",
    "        self.metrics = metrics or [Dice_f1(), Iou()]\n",
    "        self.cbs = cbs or [SaveModelCallback(monitor='iou'), ElasticDeformCallback, ShowGraphCallback]\n",
    "        for key, value in get_default_shapes(arch).items():\n",
    "            ds_kwargs.setdefault(key, value)\n",
    "        self.ds_kwargs = ds_kwargs\n",
    "        self.models = {}\n",
    "        self._set_splits()\n",
    "        self.ds = RandomTileDataset(files, label_fn=label_fn, create_weights=False, **self.mw_kwargs, **self.ds_kwargs)\n",
    "        self.in_channels = self.ds.get_data(max_n=1)[0].shape[-1]\n",
    "        self.stats = stats or self.ds.compute_stats()\n",
    "        self.batch_tfms = Normalize.from_stats(*self.stats)\n",
    "        self.df_val, self.df_ens, self.df_model = None,None,None\n",
    "                 \n",
    "    def _set_splits(self):\n",
    "        n_splits=min(self.n_splits,len(self.files))\n",
    "        kf = KFold(n_splits, shuffle=True, random_state=self.random_state)\n",
    "        self.splits = {key:(self.files[idx[0]], self.files[idx[1]]) for key, idx in zip(range(1,n_splits+1), kf.split(self.files))}       \n",
    "        \n",
    "    def fit(self, i, epochs, lr_max=5e-3, bs=4, **kwargs):\n",
    "        name = self.ensemble_dir/f'{self.arch}_model-{i}.pth'\n",
    "        print(f'Starting training for {name.name}')\n",
    "        files_train, files_val = self.splits[i]\n",
    "        train_ds = RandomTileDataset(files_train, self.label_fn, **self.mw_kwargs, **self.ds_kwargs)\n",
    "        valid_ds = TileDataset(files_val, self.label_fn, **self.mw_kwargs,**self.ds_kwargs)\n",
    "        dls = DataLoaders.from_dsets(train_ds, valid_ds, bs=bs, after_item=self.item_tfms, after_batch=self.batch_tfms)\n",
    "        model = torch.hub.load(self.repo, self.arch, pretrained=self.pretrained, n_classes=dls.c, in_channels=self.in_channels, **kwargs)\n",
    "        if torch.cuda.is_available(): dls.cuda(), model.cuda()\n",
    "        learn = Learner(dls, model, metrics=self.metrics, wd=self.wd, loss_func=self.loss_fn, opt_func=self.opt_func, cbs=self.cbs)\n",
    "        if self.mpt: learn.to_fp16()\n",
    "        learn.fit_one_cycle(epochs, lr_max)\n",
    "        \n",
    "        print(f'Saving model at {name}')    \n",
    "        name.parent.mkdir(exist_ok=True, parents=True)\n",
    "        torch.save(learn.model.state_dict(), name, _use_new_zipfile_serialization=False)\n",
    "        self.models[i]=name\n",
    "        \n",
    "    def fit_ensemble(self, epochs, skip=False, **kwargs):\n",
    "        for i in range(1, self.n+1):\n",
    "            if skip and (i in self.models): continue\n",
    "            self.fit(i, epochs,  **kwargs)\n",
    "       \n",
    "    def set_n(self, n):\n",
    "        for i in range(n, len(self.models)):\n",
    "            self.models.pop(i+1, None)            \n",
    "        self.n = n\n",
    "                 \n",
    "    def predict(self, files, model_no, bs=4, **kwargs):\n",
    "        model_path = self.models[model_no]\n",
    "        ds = TileDataset(files, **self.ds_kwargs)\n",
    "        dls = DataLoaders.from_dsets(ds, batch_size=bs, after_batch=self.batch_tfms, shuffle=False, drop_last=False)\n",
    "        model = torch.hub.load(self.repo, self.arch, pretrained=None, n_classes=dls.c, in_channels=self.in_channels, pre_ssl=False)\n",
    "        model.load_state_dict(torch.load(model_path), strict=True)\n",
    "        if torch.cuda.is_available(): dls.cuda(), model.cuda()\n",
    "        learn = Learner(dls, model, loss_func=self.loss_fn)\n",
    "        if self.mpt: learn.to_fp16()\n",
    "        results = learn.predict_tiles(dl=dls.train, use_tta=True, **kwargs)\n",
    "        pth_tmp = self.path/'.tmp'/model_path.name\n",
    "        save_tmp(pth_tmp, files, results)\n",
    "        return results\n",
    "                               \n",
    "    def get_validation_results(self, **kwargs):\n",
    "        res_list = []\n",
    "        for i in self.models:\n",
    "            _, files_val = self.splits[i]\n",
    "            res = self.predict(files_val, i, **kwargs)\n",
    "            for j, f in enumerate(files_val):\n",
    "                msk = self.ds.get_data(f, mask=True)[0]\n",
    "                m_path = self.models[i].name\n",
    "                df_tmp = pd.Series({'file' : f.name,\n",
    "                        'model' :  m_path,\n",
    "                        'img_path': f,\n",
    "                        'msk_path': self.label_fn(f),\n",
    "                        'res_path': self.path/'.tmp'/m_path/f'{f.stem}.npz',\n",
    "                        'iou': iou(msk, res[1][j]),\n",
    "                        'energy_max': res[3][j].numpy()})\n",
    "                res_list.append(df_tmp)\n",
    "        self.df_val = pd.DataFrame(res_list)\n",
    "        \n",
    "    def show_validation_results(self, file=None):\n",
    "        if self.df_val is None: self.get_validation_results()\n",
    "        df = self.df_val\n",
    "        if file is not None: df = df[df.file==file]\n",
    "        for _, r in df.iterrows():\n",
    "            img = self.ds.get_data(r.img_path)[0]\n",
    "            msk = self.ds.get_data(r.img_path, mask=True)[0]\n",
    "            with open(r.res_path, 'rb') as res:\n",
    "                tmp = np.load(res)\n",
    "                pred = tmp['seg']\n",
    "                std = tmp['std']\n",
    "            plot_results(img, msk, pred, std, df=r)        \n",
    "                \n",
    "    def get_models(self, path=None):\n",
    "        path = path or self.ensemble_dir\n",
    "        models = get_files(path, extensions='.pth', recurse=False)\n",
    "        models = [x for x in models if self.arch in x.name]\n",
    "        if len(models)>0:\n",
    "            self.models = {}\n",
    "            for m in models:\n",
    "                model_id = int(m.stem[-1])\n",
    "                self.models[model_id] = m\n",
    "            print(self.models)\n",
    "        else:\n",
    "            print(f'No {self.arch} models found in folder: {path}')\n",
    "            \n",
    "    def ensemble_results(self, files):\n",
    "        pth_out = self.path/'.tmp'/f'{self.arch}_ensemble'\n",
    "        pth_out.mkdir(exist_ok=True, parents=True)\n",
    "        res_list = []\n",
    "        for f in files:\n",
    "            for m in self.models.values():\n",
    "                pth_tmp = self.path/'.tmp'/m.name/f'{f.stem}.npz'\n",
    "                m_smx, m_std, m_enrgy = tta.Merger(), tta.Merger(), tta.Merger()\n",
    "                with open(pth_tmp, 'rb') as file:\n",
    "                    tmp = np.load(file)\n",
    "                    m_smx.append(tmp['smx'])\n",
    "                    m_std.append(tmp['std'])\n",
    "                    m_enrgy.append(tmp['enrgy'])\n",
    "            smx = m_smx.result()\n",
    "            seg = np.argmax(smx, axis=-1)\n",
    "            enrgy = m_enrgy.result()\n",
    "            np.savez(pth_out/f'{f.stem}.npz', smx=smx, seg=seg, std=m_std.result(), enrgy=enrgy)\n",
    "            df_tmp = pd.Series({'file' : f.name,\n",
    "                                'model' :  pth_out.name,\n",
    "                                'img_path': f,\n",
    "                                'res_path': pth_out/f'{f.stem}.npz',\n",
    "                                'energy_max': enrgy.numpy()})\n",
    "            res_list.append(df_tmp)\n",
    "        return pd.DataFrame(res_list)\n",
    "                            \n",
    "    def get_ensemble_results(self, new_files, **kwargs):   \n",
    "        res_list = []\n",
    "        for i in self.models:\n",
    "            res = self.predict(new_files, i, **kwargs)\n",
    "            for j, f in enumerate(new_files):\n",
    "                m_path = self.models[i].name\n",
    "                df_tmp = pd.Series({'file' : f.name,\n",
    "                                    'model_no': i, \n",
    "                                    'model' :  m_path,\n",
    "                                    'img_path': f,\n",
    "                                    'res_path': self.path/'.tmp'/m_path/f'{f.stem}.npz',\n",
    "                                    'energy_max': res[3][j].numpy()})\n",
    "                res_list.append(df_tmp)\n",
    "        self.df_models = pd.DataFrame(res_list)\n",
    "        self.df_ens  = self.ensemble_results(new_files)\n",
    "    \n",
    "    def show_ensemble_results(self, file=None, model_no=None):\n",
    "        if self.df_ens is None: assert print(\"Please run `get_ensemble_results` first.\")\n",
    "        if model_no is None: df = self.df_ens\n",
    "        else: df = self.df_models[df_models.model_no==model_no]\n",
    "        if file is not None: df = df[df.file==file]\n",
    "        for _, r in df.iterrows():\n",
    "            img = self.ds.get_data(r.img_path)[0]\n",
    "            with open(r.res_path, 'rb') as res:\n",
    "                tmp = np.load(res)\n",
    "                pred = tmp['seg']\n",
    "                std = tmp['std']\n",
    "            plot_results(img, pred, std, df=r)   \n",
    "                \n",
    "    def lr_find(self, files=None, bs=4):\n",
    "        files = files or self.files\n",
    "        train_ds = RandomTileDataset(files, self.label_fn, **self.mw_kwargs, **self.ds_kwargs)\n",
    "        dls = DataLoaders.from_dsets(train_ds,train_ds, bs=bs)\n",
    "        model = torch.hub.load(self.repo, self.arch, pretrained=self.pretrained, n_classes=dls.c, in_channels=self.in_channels)\n",
    "        if torch.cuda.is_available(): dls.cuda(), model.cuda()\n",
    "        learn = Learner(dls, model, metrics=self.metrics, wd=self.wd, loss_func=self.loss_fn, opt_func=self.opt_func)\n",
    "        if self.mpt: learn.to_fp16()\n",
    "        lr_min,lr_steep = learn.lr_find()\n",
    "        print(f\"Minimum/10: {lr_min:.2e}, steepest point: {lr_steep:.2e}\")\n",
    "        return lr_min,lr_steep     \n",
    "    \n",
    "    def clear_tmp(self):\n",
    "        try: shutil.rmtree(self.path/'.tmp')\n",
    "        except: print(\"No temporary files to delete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "add_docs(EnsembleLearner, \"Meta class to train and predict model ensembles with `n` models\",\n",
    "         fit=\"Fit model number `i`\",\n",
    "         fit_ensemble=\"Fit `i` models and `skip` existing\",\n",
    "         predict=\"Predict `files` with model at `model_path`\",\n",
    "         get_validation_results=\"Validate models on validation data and save results\",\n",
    "         show_validation_results=\"Plot results of all or `file` validation images\",\n",
    "         ensemble_results=\"Merge single model results\",\n",
    "         get_ensemble_results=\"Get models and ensemble results\", \n",
    "         show_ensemble_results=\"Show result of ensemble or `model_no`\",\n",
    "         get_models=\"Get models saved at `path`\",\n",
    "         set_n=\"Change to `n` models per ensemble\",\n",
    "         lr_find=\"Wrapper for learning rate finder\",\n",
    "         clear_tmp=\"Clear directory with temporary files\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h2 id=\"EnsembleLearner\" class=\"doc_header\"><code>class</code> <code>EnsembleLearner</code><a href=\"\" class=\"source_link\" style=\"float:right\">[source]</a></h2>\n",
       "\n",
       "> <code>EnsembleLearner</code>(**`files`**, **`label_fn`**=*`None`*, **`n`**=*`5`*, **`n_splits`**=*`5`*, **`pretrained`**=*`None`*, **`wd`**=*`0.001`*, **`repo`**=*`'matjesg/deepflash2'`*, **`arch`**=*`'unext50_deepflash2'`*, **`random_state`**=*`42`*, **`metrics`**=*`None`*, **`loss_fn`**=*`None`*, **`cbs`**=*`None`*, **`mpt`**=*`False`*, **`ds_kwargs`**=*`{}`*, **`opt_func`**=*`ranger`*, **`path`**=*`None`*, **`ensemble_dir`**=*`'ensemble'`*, **`mw_kwargs`**=*`{}`*, **`stats`**=*`None`*, **`item_tfms`**=*`[Brightness -- {'max_lighting': 0.1, 'p': 1.0, 'draw': None, 'batch': False}:\n",
       "encodes: (TensorImage,object) -> encodes\n",
       "decodes: ]`*)\n",
       "\n",
       "Meta class to train and predict model ensembles with `n` models"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(EnsembleLearner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_learner.ipynb.\n",
      "Converted 01_models.ipynb.\n",
      "Converted 02_data.ipynb.\n",
      "Converted 03_metrics.ipynb.\n",
      "Converted 04_callbacks.ipynb.\n",
      "Converted 05_losses.ipynb.\n",
      "Converted 06_utils.ipynb.\n",
      "Converted 07_tta.ipynb.\n",
      "Converted 08_gui.ipynb.\n",
      "Converted add_information.ipynb.\n",
      "Converted gt_estimation.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted model_library.ipynb.\n",
      "Converted predict.ipynb.\n",
      "Converted train-Copy1.ipynb.\n",
      "Converted train.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai",
   "language": "python",
   "name": "fastai"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
