{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learner\n",
    "\n",
    "> Learner class and traing loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import os\n",
    "from time import time\n",
    "from deepflash2.models import Unet2D\n",
    "from deepflash2.callbacks import CyclicLR\n",
    "from deepflash2.lrfinder import LRFinder\n",
    "from deepflash2 import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "class Unet_Learner:\n",
    "    \"\"\"\n",
    "    Class used to train Keras models.\n",
    "\n",
    "    model (Model): tf.keras.Model\n",
    "    train_data (Iterator): Iterator instance for training set\n",
    "    val_data (Iterator):   Iterator instance for validation set\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, train_generator, val_generator=None, model = None,\n",
    "                 opt=None, loss=None, metric=None, lr=1e-5, wd=0., \n",
    "                 checkpoint_dir = 'checkpoints',\n",
    "                 workers=1, name='U-Net', use_multiprocessing=False):\n",
    "\n",
    "        if model is not None:\n",
    "            if not isinstance(model, tf.keras.Model):\n",
    "                raise ValueError('model must be of instance tf.keras.Model')\n",
    "            self.model = model\n",
    "        else:\n",
    "            self.model = Unet2D()\n",
    "\n",
    "        self.train_generator = train_generator\n",
    "        self.val_generator = val_generator\n",
    "        self.workers = workers\n",
    "        self.use_multiprocessing = use_multiprocessing\n",
    "        self.name = name\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        if not os.path.isdir(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        self.history = None\n",
    "\n",
    "        if loss is not None:\n",
    "            self.loss = loss\n",
    "        else:\n",
    "            self.loss = {'conv_u0d-score': weighted_softmax_cross_entropy,\n",
    "                         'softmax': zero_loss}\n",
    "        if metric is not None:\n",
    "            self.metric = metric\n",
    "        else:\n",
    "            self.metric = {'softmax':\n",
    "                          [tf.keras.metrics.Recall(class_id=1),\n",
    "                          tf.keras.metrics.Precision(class_id=1),\n",
    "                          metrics.MeanIoU2(num_classes=2, class_id=1, name='IoU')\n",
    "                          ]}\n",
    "\n",
    "        if opt is not None:\n",
    "            self.opt = opt\n",
    "        else:\n",
    "            if wd > 0.:\n",
    "                self.opt = tfa.optimizers.AdamW(learning_rate=lr, weight_decay=wd)\n",
    "            else:\n",
    "                self.opt = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "\n",
    "        # Compile model\n",
    "        self._compile()\n",
    "\n",
    "    def _compile(self):\n",
    "        self.model.compile(optimizer=self.opt, loss=self.loss, metrics=self.metric)\n",
    "        return\n",
    "    \n",
    "    def save(self, name='latest'):\n",
    "        model_path = os.path.join(self.checkpoint_dir, self.name + '_' + name + '.h5')\n",
    "        self.model.save_weights(model_path)\n",
    "        print('Model weights saved:', model_path)\n",
    "    \n",
    "    def load(self, name='latest', full_path=None):\n",
    "        if full_path is not None:\n",
    "            model_path = full_path\n",
    "        else:\n",
    "            model_path = os.path.join(self.checkpoint_dir, self.name + '_' + name + '.h5')\n",
    "        self.model.load_weights(model_path)\n",
    "        print('Model weights loaded from', model_path)\n",
    "\n",
    "    def fit(self, final_epoch, max_lr,\n",
    "                          initial_epoch=0,\n",
    "                          verbose = True,\n",
    "                          snapshot_interval=1,\n",
    "                          snapshot_dir= 'checkpoints',\n",
    "                          snapshot_prefix=None,\n",
    "                          log_dir = 'logs'):\n",
    "        \"\"\"\n",
    "        Standard model training. This method can be used with any optimizer.\n",
    "        Args:\n",
    "            max_lr (float): (maximum) learning rate.\n",
    "                       It is recommended that you estimate lr yourself by\n",
    "                       running lr_finder (and lr_plot) and visually inspect plot\n",
    "                       for dramatic loss drop.\n",
    "            epochs (int): Number of epochs.  Number of epochs\n",
    "            checkpoint_folder (string): Folder path in which to save the model weights\n",
    "                                        for each epoch.\n",
    "                                        File name will be of the form:\n",
    "                                        weights-{epoch:02d}-{val_loss:.2f}.hdf5\n",
    "            verbose (bool):  verbose mode\n",
    "        \"\"\"\n",
    "\n",
    "        callbacks = []\n",
    "\n",
    "        # Tensorboard Callback\n",
    "        tb = tf.keras.callbacks.TensorBoard(log_dir= log_dir + \"/{}-{}\".format(self.name, time()))\n",
    "        callbacks.append(tb)\n",
    "\n",
    "        # ModelCheckpoint Callback\n",
    "        if snapshot_prefix is not None:\n",
    "            if not os.path.isdir(snapshot_dir):\n",
    "                os.makedirs(snapshot_dir)\n",
    "            c_path = os.path.join(\n",
    "                snapshot_dir, (snapshot_prefix if snapshot_prefix is not None else self.name))\n",
    "            cp = tf.keras.callbacks.ModelCheckpoint(c_path + \".{epoch:04d}.h5\", mode='auto', period=snapshot_interval)\n",
    "            callbacks.append()\n",
    "\n",
    "\n",
    "        hist = self.model.fit(self.train_generator,\n",
    "                              steps_per_epoch=steps_per_epoch,\n",
    "                              epochs=final_epoch,\n",
    "                              initial_epoch = initial_epoch,\n",
    "                              validation_data = self.val_generator,\n",
    "                              verbose=verbose,\n",
    "                              callbacks=callbacks)\n",
    "\n",
    "        return hist\n",
    "\n",
    "    def fit_one_cycle(self, final_epoch, max_lr,\n",
    "                      initial_epoch=0,\n",
    "                      cycle_momentum = True,\n",
    "                      verbose = True,\n",
    "                      validation_freq=1,\n",
    "                      snapshot_dir = None,\n",
    "                      log_dir = 'logs'):\n",
    "        \"\"\"\n",
    "        Train model using a version of Leslie Smith's 1cycle policy.\n",
    "        This method can be used with any optimizer.\n",
    "        Args:\n",
    "            max_lr (float): (maximum) learning rate.\n",
    "                       It is recommended that you estimate lr yourself by\n",
    "                       running lr_finder (and lr_plot) and visually inspect plot\n",
    "                       for dramatic loss drop.\n",
    "            epochs (int): Number of epochs.  Number of epochs\n",
    "            checkpoint_folder (string): Folder path in which to save the model weights\n",
    "                                        for each epoch.\n",
    "                                        File name will be of the form:\n",
    "                                        weights-{epoch:02d}-{val_loss:.2f}.hdf5\n",
    "            cycle_momentum (bool):    If True and optimizer is Adam, Nadam, or Adamax, momentum of\n",
    "                                      optimzer will be cycled between 0.95 and 0.85 as described in\n",
    "                                      https://arxiv.org/abs/1803.09820.\n",
    "                                      Only takes effect if Adam, Nadam, or Adamax optimizer is used.\n",
    "            verbose (bool):  verbose mode\n",
    "        \"\"\"\n",
    "\n",
    "        callbacks = []\n",
    "\n",
    "        # Tensorboard Callback\n",
    "        tb = tf.keras.callbacks.TensorBoard(log_dir= log_dir + \"/{}-{}\".format(self.name, time()))\n",
    "        callbacks.append(tb)\n",
    "\n",
    "        # ModelCheckpoint Callback\n",
    "        if snapshot_dir is not None:\n",
    "            os.makedirs(snapshot_dir, exist_ok=True)\n",
    "        else:\n",
    "            snapshot_dir = self.checkpoint_dir\n",
    "        c_path = os.path.join(snapshot_dir, self.name)\n",
    "        #cp = tf.keras.callbacks.ModelCheckpoint(c_path + \".{epoch:04d}.h5\", mode='auto', save_freq='epoch')\n",
    "        cp = tf.keras.callbacks.ModelCheckpoint(c_path + \"_{}_{}.h5\".format(final_epoch, max_lr),\n",
    "                                                mode='max', save_freq='epoch', monitor = 'val_softmax_IoU', \n",
    "                                                save_best_only=True, verbose=0)\n",
    "        callbacks.append(cp)\n",
    "\n",
    "        # CLR Callback\n",
    "        if cycle_momentum:\n",
    "            max_momentum = 0.95\n",
    "            min_momentum = 0.85\n",
    "        else:\n",
    "            max_momentum = None\n",
    "            min_momentum = None\n",
    "\n",
    "        #num_samples = len(train_generator)*train_generator.batch_size\n",
    "        #steps_per_epoch = np.ceil(num_samples/train_generator.batch_size)\n",
    "        steps_per_epoch = len(self.train_generator)\n",
    "        epochs = final_epoch-initial_epoch\n",
    "        # clr_fn = lambda x: 0.5*(1+np.sin(x*np.pi/2.))\n",
    "        clr = CyclicLR(base_lr=max_lr/10, max_lr=max_lr,\n",
    "                                  step_size=np.ceil((steps_per_epoch*epochs)/2), # Authors suggest setting step_size = (2-8) x (training iterations in epoch)\n",
    "                                  reduce_on_plateau=0,\n",
    "                                  max_momentum=max_momentum,\n",
    "                                  min_momentum=min_momentum,\n",
    "                                  verbose=verbose)\n",
    "\n",
    "        callbacks.append(clr)\n",
    "\n",
    "        hist = self.model.fit(self.train_generator,\n",
    "                              steps_per_epoch=steps_per_epoch,\n",
    "                              epochs=final_epoch,\n",
    "                              initial_epoch = initial_epoch,\n",
    "                              validation_data = self.val_generator,\n",
    "                              validation_freq=validation_freq,\n",
    "                              verbose=verbose,\n",
    "                              callbacks=callbacks)\n",
    "\n",
    "        hist.history['lr'] = clr.history['lr']\n",
    "        hist.history['iterations'] = clr.history['iterations']\n",
    "        if cycle_momentum:\n",
    "            hist.history['momentum'] = clr.history['momentum']\n",
    "        self.history = hist\n",
    "\n",
    "        return hist\n",
    "\n",
    "    def predict(self, tile_generator):\n",
    "        \"\"\"\n",
    "        Generates output predictions for the input samples.\n",
    "        Args:\n",
    "            tile_generator(): number of layers to freeze\n",
    "        Returns:\n",
    "            softmax_scores(list)\n",
    "            binary_segmentations(list)\n",
    "        \"\"\"\n",
    "\n",
    "        logits, softmax_score = self.model.predict(tile_generator, verbose=1)\n",
    "\n",
    "        smxcores = []\n",
    "        segmentations = []\n",
    "\n",
    "        for idx in range(len(tile_generator)):\n",
    "            outIdx = tile_generator.image_indices[idx]\n",
    "            outShape = tile_generator.image_shapes[idx]\n",
    "            outSlice = tile_generator.out_slices[idx]\n",
    "            inSlice = tile_generator.in_slices[idx]\n",
    "            if len(smxcores) < outIdx + 1:\n",
    "                smxcores.append(np.empty((*outShape, self.model.n_classes)))\n",
    "                segmentations.append(np.empty(outShape))\n",
    "            smxcores[outIdx][outSlice] = softmax_score[idx][inSlice]\n",
    "            segmentations[outIdx][outSlice] = np.argmax(softmax_score[idx], axis=-1)[inSlice]\n",
    "\n",
    "        return smxcores, segmentations\n",
    "\n",
    "\n",
    "    def freeze(self, freeze_range=None):\n",
    "        \"\"\"\n",
    "        If freeze_range is None, makes all layers trainable=False except last layer.\n",
    "        If freeze_range is given, freezes the first <freeze_range> layers and\n",
    "        unfrezes all remaining layers.\n",
    "        Args:\n",
    "            freeze_range(int): number of layers to freeze\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "\n",
    "        if freeze_range is None:\n",
    "            # freeze everything except last layer\n",
    "            for layer in self.trainModel.layers:\n",
    "                if layer.name != 'conv_u0d-score':\n",
    "                    #print(layer.name)\n",
    "                    layer.trainable=False\n",
    "\n",
    "        else:\n",
    "            # freeze all layers up to and including layer_id\n",
    "            if type(freeze_range) != type(1) or freeze_range <1:\n",
    "                raise ValueError('freeze_range must be integer > 0')\n",
    "            for i, layer in enumerate(self.trainModel.layers):\n",
    "                if i < freeze_range:\n",
    "                    layer.trainable=False\n",
    "                else:\n",
    "                    layer.trainable=True\n",
    "        self._compile()\n",
    "        return\n",
    "\n",
    "    # https://github.com/amaiya/ktrain/blob/55e00d850b8dc43adcca64d0bb4f3f00c283c907/ktrain/core.py#L262\n",
    "    def unfreeze(self, exclude_range=None):\n",
    "        \"\"\"\n",
    "        Make every layer trainable except those in exclude_range.\n",
    "        unfreeze is simply a proxy method to freeze.\n",
    "        \"\"\"\n",
    "        # make all layers trainable\n",
    "        for i, layer in enumerate(self.trainModel.layers):\n",
    "            layer.trainable = True\n",
    "        if exclude_range:\n",
    "            for i, layer in enumerate(self.trainModel.layers[:exclude_range]):\n",
    "                layer.trainable = False\n",
    "        self._compile()\n",
    "        return\n",
    "    \n",
    "    def lr_find(self, start_lr=1e-07, lr_mult=1.2, verbose=0):\n",
    "       \n",
    "        # Instantiate Learning Rate Finder\n",
    "        self.lr_finder = LRFinder(self.model, stop_factor=4)\n",
    "        # Find LR\n",
    "        self.lr_finder.find(self.train_generator, start_lr=start_lr, lr_mult=lr_mult, verbose=verbose)\n",
    "        \n",
    "        \n",
    "    def plot_loss(self):\n",
    "        self.lr_finder.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "def weighted_softmax_cross_entropy(target_y, predicted_y):\n",
    "    target_y, w = tf.split(target_y, num_or_size_splits=[-1, 1], axis=-1)\n",
    "    w = w[...,0]\n",
    "    return tf.compat.v1.losses.softmax_cross_entropy(onehot_labels = target_y,\n",
    "                                                     logits = predicted_y,\n",
    "                                                     weights=w,\n",
    "                                                     reduction=tf.compat.v1.losses.Reduction.MEAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def zero_loss(predicted_y, target_y):\n",
    "    return tf.constant([0.])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
