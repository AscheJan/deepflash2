{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Losses\n",
    "\n",
    "> Implements custom loss functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "from fastai2.vision.all import *\n",
    "#import torch\n",
    "#import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighted Softmax Cross Entropy Loss\n",
    "\n",
    "as described by Falk, Thorsten, et al. \"U-Net: deep learning for cell counting, detection, and morphometry.\" Nature methods 16.1 (2019): 67-70.\n",
    "\n",
    "\n",
    "- `axis` for softmax calculations. Defaulted at 1 (channel dimension).\n",
    "- `reduction` will be used when we call `Learner.get_preds`\n",
    "- `activation` function will be applied on the raw output logits of the model when calling `Learner.get_preds` or `Learner.predict`\n",
    "- `decodes` function converts the output of the model to a format similar to the target (here binary masks). This is used in `Learner.predict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class WeightedSoftmaxCrossEntropy(torch.nn.Module):\n",
    "    \"Weighted Softmax Cross Entropy loss functions\"\n",
    "    def __init__(self, axis=-1, *args, reduction = 'mean'):\n",
    "        super().__init__()\n",
    "        self.reduction = reduction\n",
    "        self.axis = axis\n",
    "        \n",
    "    def decodes(self, x):  \n",
    "        return x.argmax(dim=self.axis)\n",
    "\n",
    "    def activation(self, x): \n",
    "        return F.softmax(x, dim=self.axis)\n",
    "    \n",
    "    def forward(self, inputs, targ_weights): \n",
    "    \n",
    "        # Unpack targets and weights tuple\n",
    "        targets = targ_weights[0]\n",
    "        weights = targ_weights[1]\n",
    "        \n",
    "        # Weighted soft-max cross-entropy loss\n",
    "        log_smx = F.log_softmax(inputs, dim=1)*targets\n",
    "        # Broadcasting weights a axis 1 instead?\n",
    "        loss_wce = -log_smx.min(dim=1).values*weights\n",
    "        \n",
    "        if  self.reduction == 'mean':\n",
    "            return loss_wce.mean()\n",
    "            \n",
    "        elif self.reduction == 'sum':\n",
    "            return loss_wce.sum()\n",
    "        \n",
    "        else:\n",
    "            return loss_wce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a segmentation task, we want to take the softmax over the channel dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst = WeightedSoftmaxCrossEntropy(axis=1)\n",
    "output = torch.randn(4, 2, 356, 356)\n",
    "targ_weights = (torch.ones_like(output), torch.randn(4, 356, 356))\n",
    "_ = tst(output, targ_weights)\n",
    "\n",
    "test_eq(tst.activation(output), F.softmax(output, dim=1))\n",
    "test_eq(tst.decodes(output), output.argmax(dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_learner.ipynb.\n",
      "Converted 01_models.ipynb.\n",
      "Converted 02_data.ipynb.\n",
      "Converted 03_metrics.ipynb.\n",
      "Converted 04_callbacks.ipynb.\n",
      "Converted 05_losses.ipynb.\n",
      "Converted 06_utils.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
