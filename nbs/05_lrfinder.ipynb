{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp lrfinder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Rate Finder\n",
    "\n",
    "> Implementation of the LR Range test from Leslie Smith."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate Finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class LRFinder:\n",
    "    \"\"\"\n",
    "    Tracks (and plots) the change in loss of a Keras model as learning rate is gradually increased.\n",
    "    Used to visually identify a good learning rate, given model and data.\n",
    "    Reference:\n",
    "        Original Paper: https://arxiv.org/abs/1506.01186\n",
    "    \"\"\"\n",
    "    def __init__(self, model, stop_factor=4):\n",
    "        self.model = model\n",
    "        self.losses = []\n",
    "        self.lrs = []\n",
    "        self.best_loss = 1e9\n",
    "        self._weightfile = None\n",
    "        self.stop_factor = stop_factor\n",
    "\n",
    "        self.avg_loss = 0\n",
    "        self.batch_num = 0\n",
    "        self.beta = 0.98\n",
    "\n",
    "    def on_batch_end(self, batch, logs):\n",
    "        # Log the learning rate\n",
    "        lr = tf.keras.backend.get_value(self.model.optimizer.lr)\n",
    "        self.lrs.append(lr)\n",
    "\n",
    "        # Log the loss\n",
    "        loss = logs['loss']\n",
    "        self.batch_num +=1\n",
    "        self.avg_loss = self.beta * self.avg_loss + (1-self.beta) *loss\n",
    "        smoothed_loss = self.avg_loss / (1 - self.beta**self.batch_num)\n",
    "        self.losses.append(smoothed_loss)\n",
    "\n",
    "\n",
    "        # Check whether the loss got too large or NaN\n",
    "        if self.batch_num > 1 and smoothed_loss > self.stop_factor * self.best_loss:\n",
    "            self.model.stop_training = True\n",
    "            return\n",
    "\n",
    "        # record best loss\n",
    "        if smoothed_loss < self.best_loss or self.batch_num==1:\n",
    "            self.best_loss = smoothed_loss\n",
    "\n",
    "        # Increase the learning rate for the next batch\n",
    "        lr *= self.lr_mult\n",
    "        tf.keras.backend.set_value(self.model.optimizer.lr, lr)\n",
    "\n",
    "        # stop if LR grows too large\n",
    "        if lr > 10.:\n",
    "            self.model.stop_training = True\n",
    "            return\n",
    "\n",
    "\n",
    "    def find(self, train_data, start_lr, lr_mult=1.01, max_epochs=None,\n",
    "             batch_size=4, workers=1, use_multiprocessing=False, verbose=1):\n",
    "        \"\"\"\n",
    "        Track loss as learning rate is increased.\n",
    "        NOTE: batch_size is ignored when train_data is instance of Iterator.\n",
    "        \"\"\"\n",
    "\n",
    "        # check arguments and initialize\n",
    "        if train_data is None:\n",
    "            raise ValueError('train_data is required')\n",
    "        #U.data_arg_check(train_data=train_data, train_required=True)\n",
    "        self.lrs = []\n",
    "        self.losses = []\n",
    "\n",
    "        # compute steps_per_epoch\n",
    "        # num_samples = len(train_data)\n",
    "        # steps_per_epoch = num_samples // train_data.batch_size\n",
    "        steps_per_epoch = len(train_data)\n",
    "\n",
    "        # max_epochs and lr_mult are None, set max_epochs\n",
    "        # using sample size of 1500 batches\n",
    "        if max_epochs is None and lr_mult is None:\n",
    "            max_epochs = int(np.ceil(1500./steps_per_epoch))\n",
    "\n",
    "        if max_epochs:\n",
    "            epochs = max_epochs\n",
    "            num_batches = epochs * steps_per_epoch\n",
    "            end_lr = 10 if start_lr < 10 else start_lr * 10\n",
    "            self.lr_mult = (end_lr / start_lr) ** (1 / num_batches)\n",
    "        else:\n",
    "            epochs = 1024\n",
    "            self.lr_mult = lr_mult\n",
    "\n",
    "        # Save weights into a file\n",
    "        new_file, self._weightfile = tempfile.mkstemp()\n",
    "        self.model.save_weights(self._weightfile)\n",
    "\n",
    "        # Remember the original learning rate\n",
    "        original_lr = tf.keras.backend.get_value(self.model.optimizer.lr)\n",
    "\n",
    "        # Set the initial learning rate\n",
    "        tf.keras.backend.set_value(self.model.optimizer.lr, start_lr)\n",
    "\n",
    "        callback = tf.keras.callbacks.LambdaCallback(on_batch_end=lambda batch, logs: self.on_batch_end(batch, logs))\n",
    "\n",
    "        self.model.fit(train_data, steps_per_epoch=steps_per_epoch, epochs=epochs,\n",
    "                       workers=workers, use_multiprocessing=use_multiprocessing,\n",
    "                       verbose=verbose, callbacks=[callback])\n",
    "\n",
    "        # Restore the weights to the state before model fitting\n",
    "        self.model.load_weights(self._weightfile)\n",
    "        self._weightfile=None\n",
    "\n",
    "        # Restore the original learning rate\n",
    "        tf.keras.backend.set_value(self.model.optimizer.lr, original_lr)\n",
    "\n",
    "\n",
    "        return\n",
    "\n",
    "\n",
    "    def plot_loss(self, n_skip_beginning=10, n_skip_end=1):\n",
    "        \"\"\"\n",
    "        Plots the loss.\n",
    "        Parameters:\n",
    "            n_skip_beginning - number of batches to skip on the left.\n",
    "            n_skip_end - number of batches to skip on the right.\n",
    "            highlight - will highlight numerical estimate\n",
    "                        of best lr if True\n",
    "        \"\"\"\n",
    "        fig, ax = plt.subplots()\n",
    "        plt.ylabel(\"loss\")\n",
    "        plt.xlabel(\"learning rate (log scale)\")\n",
    "        ax.plot(self.lrs[n_skip_beginning:-n_skip_end], self.losses[n_skip_beginning:-n_skip_end])\n",
    "        plt.xscale('log')\n",
    "        return\n",
    "\n",
    "\n",
    "    def plot_loss_change(self, sma=1, n_skip_beginning=10, n_skip_end=5, y_lim=(-0.01, 0.01)):\n",
    "        \"\"\"\n",
    "        Plots rate of change of the loss function.\n",
    "        Parameters:\n",
    "            sma - number of batches for simple moving average to smooth out the curve.\n",
    "            n_skip_beginning - number of batches to skip on the left.\n",
    "            n_skip_end - number of batches to skip on the right.\n",
    "            y_lim - limits for the y axis.\n",
    "        \"\"\"\n",
    "        assert sma >= 1\n",
    "        derivatives = [0] * sma\n",
    "        for i in range(sma, len(self.lrs)):\n",
    "            derivative = (self.losses[i] - self.losses[i - sma]) / sma\n",
    "            derivatives.append(derivative)\n",
    "\n",
    "        plt.ylabel(\"rate of loss change\")\n",
    "        plt.xlabel(\"learning rate (log scale)\")\n",
    "        plt.plot(self.lrs[n_skip_beginning:-n_skip_end], derivatives[n_skip_beginning:-n_skip_end])\n",
    "        plt.xscale('log')\n",
    "        plt.ylim(y_lim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
