---

title: Losses


keywords: fastai
sidebar: home_sidebar

summary: "Implements popular segmentation loss functions."
description: "Implements popular segmentation loss functions."
nb_path: "nbs/05_losses.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/05_losses.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Losses implemented here:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Loss-Wrapper-functions">Loss Wrapper functions<a class="anchor-link" href="#Loss-Wrapper-functions"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Wrapper for handling different tensor types from <a href="https://docs.fast.ai/torch_core.html#TensorBase">fastai</a>.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="FastaiLoss" class="doc_header"><code>class</code> <code>FastaiLoss</code><a href="https://github.com/matjesg/deepflash2/tree/master/deepflash2/losses.py#L19" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>FastaiLoss</code>(<strong><code>loss</code></strong>, <strong><code>axis</code></strong>=<em><code>1</code></em>) :: <code>_Loss</code></p>
</blockquote>
<p>Wrapper class around loss function for handling different tensor types.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Wrapper for combining different losses, adapted from from <a href="https://github.com/BloodAxe/pytorch-toolbelt">pytorch-toolbelt</a></p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="WeightedLoss" class="doc_header"><code>class</code> <code>WeightedLoss</code><a href="https://github.com/matjesg/deepflash2/tree/master/deepflash2/losses.py#L37" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>WeightedLoss</code>(<strong><code>loss</code></strong>, <strong><code>weight</code></strong>=<em><code>1.0</code></em>) :: <code>_Loss</code></p>
</blockquote>
<p>Wrapper class around loss function that applies weighted with fixed factor.
This class helps to balance multiple losses if they have different scales</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="JointLoss" class="doc_header"><code>class</code> <code>JointLoss</code><a href="https://github.com/matjesg/deepflash2/tree/master/deepflash2/losses.py#L50" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>JointLoss</code>(<strong><code>first</code></strong>:<code>Module</code>, <strong><code>second</code></strong>:<code>Module</code>, <strong><code>first_weight</code></strong>=<em><code>1.0</code></em>, <strong><code>second_weight</code></strong>=<em><code>1.0</code></em>) :: <code>_Loss</code></p>
</blockquote>
<p>Wrap two loss functions into one. This class computes a weighted sum of two losses.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Popular-segmentation-losses">Popular segmentation losses<a class="anchor-link" href="#Popular-segmentation-losses"> </a></h2><p>The <code>get_loss()</code> function loads popular segmentation losses from <a href="https://github.com/qubvel/segmentation_models.pytorch">Segmenation Models Pytorch</a> and <a href="https://kornia.readthedocs.io/en/latest/losses.html#semantic-segmentation">kornia</a>:</p>
<ul>
<li>(Soft) CrossEntropy Loss</li>
<li>Dice Loss</li>
<li>Jaccard Loss</li>
<li>Focal Loss</li>
<li>Lovasz Loss</li>
<li>TverskyLoss</li>
</ul>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="Poly1CrossEntropyLoss" class="doc_header"><code>class</code> <code>Poly1CrossEntropyLoss</code><a href="https://github.com/matjesg/deepflash2/tree/master/deepflash2/losses.py#L62" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>Poly1CrossEntropyLoss</code>(<strong><code>num_classes</code></strong>:<code>int</code>, <strong><code>epsilon</code></strong>:<code>float</code>=<em><code>1.0</code></em>, <strong><code>reduction</code></strong>:<code>str</code>=<em><code>'mean'</code></em>) :: <code>Module</code></p>
</blockquote>
<p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>

<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))

</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<p>.. note::
    As per the example above, an <code>__init__()</code> call to the parent class
    must be made before assignment on the child.</p>
<p>:ivar training: Boolean represents whether this module is in training or
                evaluation mode.
:vartype training: bool</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">n_classes</span><span class="o">=</span><span class="mi">2</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="n">n_classes</span><span class="p">,</span> <span class="mi">356</span><span class="p">,</span> <span class="mi">356</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_classes</span><span class="p">,</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">356</span><span class="p">,</span> <span class="mi">356</span><span class="p">))</span>

<span class="n">tst</span> <span class="o">=</span> <span class="n">Poly1CrossEntropyLoss</span><span class="p">(</span><span class="n">num_classes</span><span class="o">=</span><span class="n">n_classes</span><span class="p">)</span> 
<span class="n">loss</span> <span class="o">=</span> <span class="n">tst</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="get_loss" class="doc_header"><code>get_loss</code><a href="https://github.com/matjesg/deepflash2/tree/master/deepflash2/losses.py#L99" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>get_loss</code>(<strong><code>loss_name</code></strong>, <strong><code>mode</code></strong>=<em><code>'multiclass'</code></em>, <strong><code>classes</code></strong>=<em><code>[1]</code></em>, <strong><code>smooth_factor</code></strong>=<em><code>0.0</code></em>, <strong><code>alpha</code></strong>=<em><code>0.5</code></em>, <strong><code>beta</code></strong>=<em><code>0.5</code></em>, <strong><code>gamma</code></strong>=<em><code>2.0</code></em>, <strong><code>reduction</code></strong>=<em><code>'mean'</code></em>, <strong>**<code>kwargs</code></strong>)</p>
</blockquote>
<p>Load losses from based on loss_name</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">n_classes</span> <span class="o">=</span> <span class="mi">2</span>
<span class="c1">#output = TensorImage(torch.randn(4, n_classes, 356, 356, requires_grad=True))</span>
<span class="c1">#target = TensorMask(torch.randint(0, n_classes, (4, 356, 356)))</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="n">n_classes</span><span class="p">,</span> <span class="mi">356</span><span class="p">,</span> <span class="mi">356</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_classes</span><span class="p">,</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">356</span><span class="p">,</span> <span class="mi">356</span><span class="p">))</span>
<span class="k">for</span> <span class="n">loss_name</span> <span class="ow">in</span> <span class="n">LOSSES</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Testing </span><span class="si">{</span><span class="n">loss_name</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="n">tst</span> <span class="o">=</span> <span class="n">get_loss</span><span class="p">(</span><span class="n">loss_name</span><span class="p">,</span> <span class="n">classes</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">n_classes</span><span class="p">)))</span> 
    <span class="n">loss</span> <span class="o">=</span> <span class="n">tst</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Testing CrossEntropyLoss
Testing DiceLoss
Testing SoftCrossEntropyLoss
Testing CrossEntropyDiceLoss
Testing JaccardLoss
Testing FocalLoss
Testing LovaszLoss
Testing TverskyLoss
Installing kornia. Please wait.
Collecting kornia
  Downloading kornia-0.6.4-py2.py3-none-any.whl (493 kB)
Requirement already satisfied: torch&gt;=1.8.1 in /home/magr/.conda/envs/fastai2/lib/python3.9/site-packages (from kornia) (1.10.2)
Requirement already satisfied: packaging in /home/magr/.local/lib/python3.9/site-packages (from kornia) (21.3)
Requirement already satisfied: typing_extensions in /home/magr/.conda/envs/fastai2/lib/python3.9/site-packages (from torch&gt;=1.8.1-&gt;kornia) (3.10.0.2)
Requirement already satisfied: pyparsing!=3.0.5,&gt;=2.0.2 in /home/magr/.local/lib/python3.9/site-packages (from packaging-&gt;kornia) (3.0.7)
Installing collected packages: kornia
Successfully installed kornia-0.6.4
Testing Poly1CrossEntropyLoss
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">ce1</span> <span class="o">=</span> <span class="n">get_loss</span><span class="p">(</span><span class="s1">&#39;SoftCrossEntropyLoss&#39;</span><span class="p">,</span> <span class="n">smooth_factor</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">ce2</span> <span class="o">=</span> <span class="n">CrossEntropyLossFlat</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">test_close</span><span class="p">(</span><span class="n">ce1</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">),</span> <span class="n">ce2</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">),</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-04</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">jc</span> <span class="o">=</span> <span class="n">get_loss</span><span class="p">(</span><span class="s1">&#39;JaccardLoss&#39;</span><span class="p">)</span>
<span class="n">dc</span> <span class="o">=</span> <span class="n">get_loss</span><span class="p">(</span><span class="s1">&#39;DiceLoss&#39;</span><span class="p">)</span>
<span class="n">dc_loss</span> <span class="o">=</span> <span class="n">dc</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="n">dc_to_jc</span> <span class="o">=</span> <span class="mi">2</span><span class="o">*</span><span class="n">dc_loss</span><span class="o">/</span><span class="p">(</span><span class="n">dc_loss</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="c1">#it seems to be the other way around?</span>
<span class="n">test_close</span><span class="p">(</span><span class="n">jc</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">),</span> <span class="n">dc_to_jc</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-02</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tw</span> <span class="o">=</span> <span class="n">get_loss</span><span class="p">(</span><span class="s2">&quot;TverskyLoss&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">test_close</span><span class="p">(</span><span class="n">dc</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">),</span> <span class="n">tw</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">),</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-02</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="n">n_classes</span><span class="p">,</span> <span class="mi">356</span><span class="p">,</span> <span class="mi">356</span><span class="p">)</span>
<span class="n">output</span><span class="p">[:,</span><span class="mi">1</span><span class="p">,</span><span class="o">...</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.5</span>
<span class="n">tst</span> <span class="o">=</span> <span class="n">get_loss</span><span class="p">(</span><span class="n">loss_name</span><span class="o">=</span><span class="s1">&#39;DiceLoss&#39;</span><span class="p">,</span> <span class="n">classes</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span> 
<span class="n">tst2</span> <span class="o">=</span> <span class="n">get_loss</span><span class="p">(</span><span class="n">loss_name</span><span class="o">=</span><span class="s1">&#39;DiceLoss&#39;</span><span class="p">,</span> <span class="n">classes</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">n_classes</span><span class="p">)))</span> 
<span class="n">test_ne</span><span class="p">(</span><span class="n">tst</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">),</span> <span class="n">tst2</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

</div>
 

