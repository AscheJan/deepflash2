---

title: Title


keywords: fastai
sidebar: home_sidebar



nb_path: "nbs/index.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/index.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="https://raw.githubusercontent.com/matjesg/deepflash2/master/nbs/media/logo/deepflash2_logo_medium.png" alt="deepflash2"></p>
<p>Official repository of deepflash2 - a deep-learning pipeline for segmentation of ambiguous microscopic images.</p>
<p><img src="https://github.com/matjesg/deepflash2/workflows/CI/badge.svg" alt="CI"> 
<a href="https://pypi.org/project/deepflash2/#description"><img src="https://img.shields.io/pypi/v/deepflash2?color=blue&amp;label=pypi%20version" alt="PyPI"></a> 
<a href="https://pypistats.org/packages/deepflash2"><img src="https://img.shields.io/pypi/dm/deepflash2" alt="PyPI - Downloads"></a>
<a href="https://anaconda.org/matjesg/deepflash2"><img src="https://img.shields.io/conda/vn/matjesg/deepflash2?color=seagreen&amp;label=conda%20version" alt="Conda (channel only)"></a> 
<a href="https://github.com/matjesg/deepflash2"><img src="https://github.com/matjesg/deepflash2/workflows/Build%20deepflash2%20images/badge.svg" alt="Build fastai images"></a>
<a href="https://github.com/matjesg/deepflash2/"><img src="https://img.shields.io/github/stars/matjesg/deepflash2?style=social" alt="GitHub stars"></a>
<a href="https://github.com/matjesg/deepflash2/"><img src="https://img.shields.io/github/forks/matjesg/deepflash2?style=social" alt="GitHub forks"></a></p>
<hr>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>The best of two worlds:</strong>
Combining state-of-the-art deep learning with a barrier free environment for life science researchers.</p>
<blockquote><p>Read the <a href="https://arxiv.org/abs/2111.06693">paper</a>, watch the <a href="https://matjesg.github.io/deepflash2/tutorial.html">tutorials</a>, or read the <a href="https://matjesg.github.io/deepflash2/">docs</a>.</p>
<ul>
<li><strong>No coding skills required</strong> (graphical user interface)</li>
<li><strong>Ground truth estimation</strong> from the annotations of multiple experts for model training and validation</li>
<li><strong>Quality assurance and out-of-distribution detection</strong> for reliable prediction on new data </li>
<li><strong>Best-in-class performance</strong> for semantic and instance segmentation</li>
</ul>
</blockquote>
<p><img src="https://github.com/matjesg/deepflash2/blob/master/nbs/media/sample_images.png?raw=true" width="800px" style="max-width: 800pxpx"></p>
<p><img style="float: left;padding: 0px 10px 0px 0px;" src="https://www.kaggle.com/static/images/medals/competitions/goldl@1x.png"></p>
<p><strong>Kaggle Gold Medal and Innovation Price Winner:</strong> The <em>deepflash2</em> Python API built the foundation for winning the <a href="https://hubmapconsortium.github.io/ccf/pages/kaggle.html">Innovation Award</a> a Kaggle Gold Medal in the <a href="https://www.kaggle.com/c/hubmap-kidney-segmentation">HuBMAP - Hacking the Kidney</a> challenge. 
Have a look at our <a href="https://www.kaggle.com/matjes/hubmap-deepflash2-judge-price">solution</a></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Quick-Start-and-Demo">Quick Start and Demo<a class="anchor-link" href="#Quick-Start-and-Demo"> </a></h2><blockquote><p>Get started in less than a minute. Watch the <a href="https://matjesg.github.io/deepflash2/tutorial.html">tutorials</a> for help.</p>
</blockquote>
<p>For a quick start, run <em>deepflash2</em> in Google Colaboratory with free access to graphics processing units (GPUs).</p>
<p><a href="https://colab.research.google.com/github/matjesg/deepflash2/blob/master/deepflash2_GUI.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Colab"></a></p>
<p>To try the functionalities of <em>deepflash2</em>, open the <em>deepflash2</em> GUI in <a href="(https://colab.research.google.com/github/matjesg/deepflash2/blob/master/deepflash2_GUI.ipynb">Colab</a> or follow the installation instructions below. The GUI provides a build-in use for sample data. After starting the GUI, select the task (GT Estimation, Training, or Prediction) and click <code>Load Sample Data</code>. For futher instructions watch the <a href="https://matjesg.github.io/deepflash2/tutorial.html">tutorials</a>.</p>
<p>We provide an overview of the tasks below:</p>
<table>
<thead><tr>
<th></th>
<th>Ground Truth (GT) Estimation</th>
<th>Training</th>
<th>Prediction</th>
</tr>
</thead>
<tbody>
<tr>
<td>Main Task</td>
<td>STAPLE or Majority Voting</td>
<td>Ensemble training  and validation</td>
<td>Semantic and instance segmentation</td>
</tr>
<tr>
<td>Sample Data</td>
<td>5 masks from 5 experts each</td>
<td>5 image/mask pairs</td>
<td>5 images and 2 trained models</td>
</tr>
<tr>
<td>Expected Output</td>
<td>5 GT Segmentation Masks</td>
<td>5 models</td>
<td>5 predicted segmentation masks  (semantic and instance) and uncertainty maps</td>
</tr>
<tr>
<td>Estimated Time</td>
<td>~ 1 min</td>
<td>~ 150 min</td>
<td>~ 4 min</td>
</tr>
</tbody>
</table>
<p>Times are estimated for Google Colab (with free NVIDIA Tesla K80 GPU). You can download the sample data <a href="https://github.com/matjesg/deepflash2/releases/tag/sample_data">here</a>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Paper-and-Experiments">Paper and Experiments<a class="anchor-link" href="#Paper-and-Experiments"> </a></h2><p>We provide a complete guide to reproduce our experiments using the <em>deepflash2 Python API</em> <a href="https://github.com/matjesg/deepflash2/tree/master/paper">here</a>. The data is currently available on <a href="xxx">Google Drive</a>.</p>
<p>The preprint of our paper is available on <a href="https://arxiv.org/abs/2111.06693">arXiv</a>. Please cite</p>

<pre><code>@misc{griebel2021deepflash2,
    title={Deep-learning in the bioimaging wild: Handling ambiguous data with deepflash2}, 
    author={Matthias Griebel and Dennis Segebarth and Nikolai Stein and Nina Schukraft and Philip Tovote and Robert Blum and Christoph M. Flath},
    year={2021},
    eprint={2111.06693},
    archivePrefix={arXiv}
}</code></pre>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="System-requirements">System requirements<a class="anchor-link" href="#System-requirements"> </a></h2><blockquote><p>Works in the browser or on your local pc/server</p>
</blockquote>
<p><em>deepflash2</em> is designed to run on Windows, Linux, or Mac (x86-64) if <a href="https://pytorch.org/get-started/locally/">pytorch</a> is installable.
We generally recommend using Google Colab as it only requires a Google Account and a device with a web browser. 
To run <em>deepflash2</em> locally, we recommend using a system with a GPU (e.g., 2 CPUs, NVIDIA Tesla K80 GPU or better).</p>
<p>Software dependencies are defined in the <a href="https://github.com/matjesg/deepflash2/blob/master/settings.ini">settings.ini</a> file. Additionally, the ground truth estimation functionalities are based on simpleITK&gt;=2.0 and the instance segmentation capabilities are complemented using cellpose v0.6.6.dev13+g316927e.</p>
<p><em>deepflash2</em> is tested on Google Colab (Ubuntu 18.04.5 LTS) and locally (Ubuntu 20.04 LTS, Windows 10, MacOS 12.0.1).</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Installation-Guide">Installation Guide<a class="anchor-link" href="#Installation-Guide"> </a></h2><blockquote><p>Typical install time is about 1-5 minutes, depending on your internet connection</p>
</blockquote>
<p>The GUI of <em>deepflash2</em> runs as a web application inside a Jupyter Notebook, the de-facto standard of computational notebooks in the scientific community. The GUI is built on top of the <em>deepflash2</em> Python API, which can be used independently (read the <a href="https://matjesg.github.io/deepflash2/">docs</a>).</p>
<h4 id="Google-Colab">Google Colab<a class="anchor-link" href="#Google-Colab"> </a></h4><p>Excute the <code>Set up environment</code> cell or follow the <code>pip</code> instructions.</p>
<p><a href="https://colab.research.google.com/github/matjesg/deepflash2/blob/master/deepflash2_GUI.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Colab"></a></p>
<h4 id="Other-systems">Other systems<a class="anchor-link" href="#Other-systems"> </a></h4><h5 id="conda"><a href="https://docs.conda.io/en/latest/">conda</a><a class="anchor-link" href="#conda"> </a></h5><p>We recommend installation into a new, clean <a href="https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html">environment</a>.</p>
<div class="highlight"><pre><span></span>conda install -c conda-forge -c fastchan -c matjesg deepflash2
</pre></div>
<h5 id="pip"><a href="https://pip.pypa.io/en/stable/">pip</a><a class="anchor-link" href="#pip"> </a></h5><p>For GPU support we recommend to install PyTorch first by following the <a href="https://pytorch.org/get-started/locally/">installation instructions</a>.</p>
<div class="highlight"><pre><span></span>pip install deepflash2
</pre></div>
<p>If you want to use the GUI, make sure to download the GUI notebook and start a Jupyter server.</p>
<div class="highlight"><pre><span></span>curl -o deepflash2_GUI.ipynb https://raw.githubusercontent.com/matjesg/deepflash2/master/deepflash2_GUI.ipynb
jupyter notebook
</pre></div>
<p>Then, open <code>deepflash2_GUI.ipynb</code> within Notebook environment.</p>
<h5 id="Docker">Docker<a class="anchor-link" href="#Docker"> </a></h5><p>Docker images for <strong>deepflash2</strong> are built on top of <a href="https://hub.docker.com/r/pytorch/pytorch/">the latest pytorch image</a>.</p>
<ul>
<li>CPU only<blockquote><p><code>docker run -p 8888:8888 matjes/deepflash2 ./run_jupyter.sh</code></p>
</blockquote>
</li>
<li>For training, we recommend to run docker with GPU support (You need to install <a href="https://github.com/NVIDIA/nvidia-docker">Nvidia-Docker</a> to enable gpu compatibility with these containers.)<blockquote><p><code>docker run --gpus all --shm-size=256m -p 8888:8888 matjes/deepflash2 ./run_jupyter.sh</code></p>
</blockquote>
</li>
</ul>
<p>All docker containers are configured to start a jupyter server. To add data, we recomment using <a href="https://docs.docker.com/storage/bind-mounts/">bind mounts</a> with <code>/workspace</code> as target. To start the GUI, open <code>deepflash2_GUI.ipynb</code> within Notebook environment.</p>
<p>For more information on how to run docker see <a href="https://docs.docker.com/get-started/">docker orientation and setup</a>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Creating-segmentation-masks-with-Fiji/ImageJ">Creating segmentation masks with Fiji/ImageJ<a class="anchor-link" href="#Creating-segmentation-masks-with-Fiji/ImageJ"> </a></h2><p>If you don't have labelled training data available, you can use this <a href="https://github.com/matjesg/DeepFLaSH/raw/master/ImageJ/create_maps_howto.pdf">instruction manual</a> for creating segmentation maps.
The ImagJ-Macro is available <a href="https://raw.githubusercontent.com/matjesg/DeepFLaSH/master/ImageJ/Macro_create_maps.ijm">here</a>.</p>

</div>
</div>
</div>
</div>
 

