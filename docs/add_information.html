---

title: Additional Information


keywords: fastai
sidebar: home_sidebar

summary: "This Notebook contains information on use of _deepflash2_."
description: "This Notebook contains information on use of _deepflash2_."
nb_path: "nbs/add_information.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/add_information.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Required-Data-Structure-and-Naming">Required Data Structure and Naming<a class="anchor-link" href="#Required-Data-Structure-and-Naming"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Ground-Truth-Estimation">Ground Truth Estimation<a class="anchor-link" href="#Ground-Truth-Estimation"> </a></h3><p><strong>Input Details</strong>:  <em>deepflash2</em> fuses</p>
<ul>
<li>binary segmentations of an image, that is, there must be a single foreground value that represents positively classified pixels<ul>
<li>Segmentation pixel values: background-class: 0; foreground-class: 1 or 255</li>
</ul>
</li>
<li>instance segmentations of an image (instances represent positively classified pixels)<ul>
<li>Segmentation pixel values: background-class: 0; foreground-instances: 1,2,...,I</li>
</ul>
</li>
</ul>
<p>Examplary input folder structure:</p>
<blockquote>
<pre><code>expert_segmentations  -&gt; one parent folder
│                     
│───expert1           -&gt; one folder per expert
│   │   mask1.png     -&gt; segmentation masks
│   │   mask2.png
│   
└───expert2
    │   mask1.png
    │   mask2.png</code></pre>
</blockquote>
<p>All common image formats (tif, png, etc.) are supported. See <a href="https://imageio.readthedocs.io/en/stable/formats/index.html">imageio docs</a>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Training">Training<a class="anchor-link" href="#Training"> </a></h3><ul>
<li><strong>Images must have unique name or ID</strong><ul>
<li>_0001.tif --&gt; name/ID: 0001; img_5.png --&gt; name/ID: img<em>5, ...</em> </li>
<li>Arbitrary number of channels (e.g., 1 greyscale; 3 RGB)</li>
</ul>
</li>
<li><strong>Corresponding masks must start with name or ID + a mask suffix__</strong><ul>
<li><strong>Semantic segmentation mask pixel values</strong>: background-class: 0; foreground-classes: 1,2,...,C (or 255 if binary)</li>
<li><strong>Instance segmentation mask pixel values (binary only)</strong>: background-class: 0; foreground-instances: 1,2,...,I</li>
<li>_0001 -&gt; 0001_mask.png (mask_suffix = "<em>mask.png")</em></li>
<li>_0001 -&gt; 0001.png (mask<em>suffix = ".png")</em></li>
<li>mask suffix is inferred automatically </li>
<li>binary segmentations of an image, that is, there must be a single foreground value that represents positively classified pixels</li>
<li>instance segmentations of an image (instances represent positively classified pixels</li>
</ul>
</li>
</ul>
<p>Examplary input folder structure:</p>
<blockquote><p>```<br>
──images            -&gt; one image folder
  │   0001.tif<br>
  │   0002.tif</p>
</blockquote>
<p>──masks             -&gt; one masks folder
  │   0001_mask.png
  │   0002_mask.png
```</p>
<p>All common image formats (tif, png, etc.) are supported. See <a href="https://imageio.readthedocs.io/en/stable/formats/index.html">imageio docs</a>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Prediction">Prediction<a class="anchor-link" href="#Prediction"> </a></h3><ul>
<li><strong>One folder for images</strong><ul>
<li>Images must have unique name or ID<ul>
<li>_0001.tif --&gt; name/ID: 0001; img_5.png --&gt; name/ID: img<em>5, ...</em> </li>
</ul>
</li>
<li>Same number of channels as training images (e.g., 1 greyscale; 3 RGB)</li>
</ul>
</li>
<li><strong>For evaluation: Corresponding masks must start with name or ID + a mask suffix</strong><ul>
<li>same requirements as for <a href="https://matjesg.github.io/deepflash2/add_information.html#Training">training</a></li>
</ul>
</li>
<li><strong>One folder containing trained models (ensemble)</strong><ul>
<li>Ensemble folder and models will be created during Training<ul>
<li>Do not change the naming of the models</li>
<li>If you want to train different ensembles, simply rename the ensemble folder</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>Examplary input folder structure:
&gt;</p>

<pre><code>──images            -&gt; one image folder
  │   0001.tif      
  │   0002.tif

──masks             -&gt; one masks folder (evaluation only)
  │   0001_mask.png
  │   0002_mask.png

──ensemble          -&gt; one model folder
  │   Unet_resnet34_2classes-fold1.pth
  │   Unet_resnet34_2classes-fold2.pth</code></pre>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Train-validation-split">Train-validation-split<a class="anchor-link" href="#Train-validation-split"> </a></h2><p>The train-validation-split is defined as _<a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html">k-fold cross validation</a>_ with <code>n_splits</code></p>
<ul>
<li><code>n_splits</code> is the minimum of: (number of files in dataset,  <code>max_splits</code> (default:5))</li>
<li>By default, the number of models per ensemble is limited to <code>n_splits</code></li>
</ul>
<p><em>Example for a dataset containing 15 images</em></p>
<ul>
<li><code>model_1</code> is trained on 12 images (3 validation images) </li>
<li><code>model_2</code> is trained on 12 images (3 different validation images) </li>
<li>...</li>
<li><code>model_5</code> is trained on 12 images (3 different validation images) </li>
</ul>
<p><em>Example for a dataset containing 2 images</em></p>
<ul>
<li><code>model_1</code> is trained on 1 image (1 validation image) </li>
<li><code>model_2</code> is trained on 1 images (1 different validation image) </li>
<li>Only two models per ensemble</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Training-Epochs-and-Iterations">Training Epochs and Iterations<a class="anchor-link" href="#Training-Epochs-and-Iterations"> </a></h2><p>To streamline the training process and allow an easier comparison across differently sized datasets, we decided to use the number of training <em>iterations</em> instead of <em>epochs</em> to define the lenght of a <a href="https://matjesg.github.io/deepflash2/utils.html#calc_iterations">training cycle</a>.</p>
<p>Some useful definitions (adapted from <a href="https://stackoverflow.com/questions/4752626/epoch-vs-iteration-when-training-neural-networks">stackoverflow</a>):</p>
<ul>
<li>Epoch: one training pass (forward pass and one backward pass) of all the training examples</li>
<li>Batch size: the number of training examples in one forward/backward pass. The higher the batch size, the more memory space you'll need.</li>
<li>Iteration: One forward pass and one backward pass using [batch size] number of examples.</li>
</ul>

</div>
</div>
</div>
</div>
 

