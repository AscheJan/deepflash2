# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/00_core.ipynb (unless otherwise specified).

__all__ = ['Unet2D', 'weighted_softmax_cross_entropy', 'zero_loss']

# Cell
import numpy as np
import tensorflow as tf
from tensorflow.keras import layers
from functools import partial

# Cell
class Unet2D:

    def __init__(self, n_channels=1, n_classes=2, n_levels=4,
               n_features=64, n_batch_norm_levels=0, upsample = False,
               relu_alpha=0.1, k_init="he_normal"):

        self.n_channels = n_channels
        self.n_levels = n_levels
        self.n_classes = n_classes
        self.n_features = n_features
        self.n_batch_norm_levels = n_batch_norm_levels
        self.relu_alpha = relu_alpha
        self.k_init = k_init
        self.upsample = upsample

        self.model = self._createModel()


    def _createModel(self):

        conf2d = partial(layers.Conv2D, padding="valid", kernel_initializer=self.k_init)
        conf2dT = partial(layers.Conv2DTranspose, padding="valid", kernel_initializer=self.k_init)

        data = layers.Input(shape=(None, None, self.n_channels), name="data")

        down_stack = []
        # Modules of the analysis path consist of two convolutions and max pooling
        for l in range(self.n_levels):
            x = conf2d(2**l * self.n_features, 3, name="conv_d{}a-b".format(l))(data if l == 0 else x)
            if l > self.n_batch_norm_levels:
                x = layers.BatchNormalization(axis=-1)(x)
            x = layers.LeakyReLU(alpha=self.relu_alpha)(x)
            x = conf2d(2**l * self.n_features, 3, name="conv_d{}b-c".format(l))(x)
            if l > self.n_batch_norm_levels:
                x = layers.BatchNormalization(axis=-1)(x)
            x = layers.LeakyReLU(alpha=self.relu_alpha)(x)
            if l >= 2:
                x = layers.Dropout(0.5)(x)
            down_stack.append(x)
            x = layers.MaxPooling2D(pool_size=(2, 2))(x)

        # Deepest layer has two convolutions only
        x = conf2d(2**self.n_levels * self.n_features, 3, name="conv_d{}a-b".format(self.n_levels))(x)
        if l > self.n_batch_norm_levels:
            x = layers.BatchNormalization(axis=-1)(x)
        x = layers.LeakyReLU(alpha=self.relu_alpha)(x)
        x = conf2d(2**self.n_levels * self.n_features, 3, name="conv_d{}b-c".format(self.n_levels))(x)
        if l > self.n_batch_norm_levels:
            x = layers.BatchNormalization(axis=-1)(x)
        x = layers.LeakyReLU(alpha=self.relu_alpha)(x)
        pad = 8

        # Modules in the synthesis path consist of up-convolution,
        # concatenation and two convolutions
        for l in range(self.n_levels - 1, -1, -1):
            name = "upconv_{}{}{}_u{}a".format(
                *(("d", l+1, "c", l) if l == self.n_levels - 1 else ("u", l+1, "d", l)))
            if self.upsample:
                x = layers.UpSampling2D(size=(2, 2), name=name)(x)
            else:
                x = conf2dT(2**np.max((l, 1)) * self.n_features, (2, 2), strides=2, name=name)(x)
                x = layers.LeakyReLU(alpha=self.relu_alpha)(x)
            x = layers.Concatenate()([layers.Cropping2D(cropping=int(pad / 2))(down_stack[l]), x])

            x = conf2d(2**np.max((l, 1)) * self.n_features, 3, name="conv_u{}b-c".format(l))(x)
            if l > self.n_batch_norm_levels:
                x = layers.BatchNormalization(axis=-1)(x)
            x = layers.LeakyReLU(alpha=self.relu_alpha)(x)
            x = conf2d(2**np.max((l, 1)) * self.n_features, 3, name="conv_u{}c-d".format(l))(x)
            if l > self.n_batch_norm_levels:
                x = layers.BatchNormalization(axis=-1)(x)
            x = layers.LeakyReLU(alpha=self.relu_alpha)(x)
            pad = 2 * (pad + 8)

        score = conf2d(self.n_classes, 1, name="conv_u0d-score")(x)
        softmax_score = layers.Softmax(name='softmax')(score)

        model= tf.keras.Model(inputs=data, outputs=[score, softmax_score])

        return model

# Cell
def weighted_softmax_cross_entropy(target_y, predicted_y):
    target_y, w = tf.split(target_y, num_or_size_splits=[-1, 1], axis=-1)
    w = w[...,0]
    return tf.compat.v1.losses.softmax_cross_entropy(onehot_labels = target_y,
                                                     logits = predicted_y,
                                                     weights=w,
                                                     reduction=tf.compat.v1.losses.Reduction.MEAN)

# Cell
def zero_loss(predicted_y, target_y):
    return tf.zeros(0)