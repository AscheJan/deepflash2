{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3_performance_comparison.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/matjesg/deepflash2/blob/master/paper/3_performance_comparison.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJ0pRdHQGAPl"
      },
      "source": [
        "# deepflash2 - Performance Comparison\n",
        "\n",
        "> This notebook calculates the performance metrics for the methods in the deepflash2 [paper](https://arxiv.org/abs/2111.06693).\n",
        "\n",
        "- **Data and results**: The data and results of the different methods are available on [Google Drive](https://drive.google.com/drive/folders/1r9AqP9qW9JThbMIvT0jhoA5mPxWEeIjs?usp=sharing). To use the data in Google Colab, create a [shortcut](https://support.google.com/drive/answer/9700156?hl=en&co=GENIE.Platform%3DDesktop) of the data folder in your personal Google Drive.\n",
        "\n",
        "*Source files created with this notebook*\n",
        "- `semantic_segmentation_results.csv`\n",
        "- `instance_segmentation_results.csv`\n",
        "- `instance_segmentation_results_agg.csv`\n",
        "\n",
        "The preceding segmentation results can be reproduced using the `train-and-predict` notebooks on [github](https://github.com/matjesg/deepflash2/paper).\n",
        "\n",
        "*References*:\n",
        "\n",
        "Griebel, M., Segebarth, D., Stein, N., Schukraft, N., Tovote, P., Blum, R., & Flath, C. M. (2021). Deep-learning in the bioimaging wild: Handling ambiguous data with deepflash2. arXiv preprint arXiv:2111.06693.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFQlTlPoKBM9"
      },
      "source": [
        "## Setup\n",
        "\n",
        "- Install dependecies\n",
        "- Connect to drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DK5ySSWKESd"
      },
      "source": [
        "!pip install -Uq deepflash2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbDoXA3PKNAm"
      },
      "source": [
        "# Imports\n",
        "import imageio\n",
        "import tifffile\n",
        "import cv2\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from fastprogress import progress_bar\n",
        "from deepflash2.all import *\n",
        "from deepflash2.data import _read_msk\n",
        "from skimage.segmentation import relabel_sequential\n",
        "check_cellpose_installation()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNwYiX9RmiMX"
      },
      "source": [
        "# Connect to drive\n",
        "try:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/gdrive')\n",
        "except:\n",
        "  print('Google Drive is not available.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2ehFoOoJIox"
      },
      "source": [
        "## Settings\n",
        "\n",
        "For sementic and instance segmentation results. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8LFII70JHOl"
      },
      "source": [
        "DATASETS_SEMANTIC_SEG = ['PV_in_HC', 'cFOS_in_HC', 'mScarlet_in_PAG', 'YFP_in_CTX', 'GFAP_in_HC']\n",
        "METHODS_SEMANTIC_SEG = ['otsu', 'unet_2019', 'nnunet', 'deepflash2']\n",
        "\n",
        "DATASETS_INSTANCE_SEG = ['PV_in_HC', 'cFOS_in_HC', 'mScarlet_in_PAG', 'YFP_in_CTX']\n",
        "METHODS_INSTANCE_SEG = ['cellpose', 'cellpose_single', 'cellpose_ensemble', 'unet_2019', 'nnunet', 'deepflash2']\n",
        "\n",
        "#https://github.com/cocodataset/cocoapi/blob/master/PythonAPI/pycocotools/cocoeval.py\n",
        "thresholds = np.linspace(.5, 0.95, int(np.round((0.95 - .5) / .05)) + 1, endpoint=True)\n",
        "\n",
        "OUTPUT_PATH = Path(\"/content/\")\n",
        "DATA_PATH = Path('/gdrive/MyDrive/deepflash2-paper')\n",
        "\n",
        "SUBDIR = 'test'\n",
        "\n",
        "min_pixel_dict = {\n",
        "    'PV_in_HC':61, \n",
        "    'cFOS_in_HC':30, \n",
        "    'mScarlet_in_PAG':385, \n",
        "    'YFP_in_CTX':193,\n",
        "}\n",
        "\n",
        "cellpose_dict = {\n",
        "    'PV_in_HC':'cyto', \n",
        "    'cFOS_in_HC':'cyto2',\n",
        "    'mScarlet_in_PAG':'cyto2',\n",
        "    'YFP_in_CTX':'cyto',\n",
        "}\n",
        "\n",
        "def repetition_mapper(x, method, dataset):\n",
        "  'Returns correct subfolder for non-trainable methods'\n",
        "  if  method=='otsu': x = 'default'\n",
        "  if  method=='cellpose': x = cellpose_dict[dataset]\n",
        "  return str(x)\n",
        "\n",
        "def expert_comparison(df, metric):\n",
        "  'Calculates expert comparison metrics on data frame'\n",
        "  df['expert_comparison'] = 'in expert range'\n",
        "  df.loc[df[metric]>df['expert_max'], 'expert_comparison'] = 'above best expert'\n",
        "  df.loc[df[metric]<df['expert_min'], 'expert_comparison'] = 'below worst expert'\n",
        "  return df\n",
        "\n",
        "def clean_labels(label_msk, min_pixel):\n",
        "  'Remove areas blow below threshold'\n",
        "  # remove areas < min pixel\n",
        "  unique, counts = np.unique(label_msk, return_counts=True)\n",
        "  label_msk[np.isin(label_msk, unique[counts<min_pixel])] = 0\n",
        "\n",
        "  # re-label image\n",
        "  label_msk, _ , _ = relabel_sequential(label_msk, offset=1)\n",
        "\n",
        "  return label_msk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbql-zGyF93s"
      },
      "source": [
        "## Metrics\n",
        "\n",
        "We propose a two-step evaluation:\n",
        "\n",
        "1. Calculation of performance metrics (method vs. estimated ground truth)\n",
        "  - Dice score for instance segmentation\n",
        "  - Mean average precision for semantic segmentation\n",
        "  - Average precision at IoU_50 for detection (supplement only)\n",
        "2. Comparison to expert performance (against estimated ground truth)\n",
        "  - Accounts for the ambiguity in the data\n",
        "\n",
        "All results are calculated on the hold-out test sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16ugKa2eQhq9"
      },
      "source": [
        "# Semantic segmentation\n",
        "results_semantic = []\n",
        "metric = 'dice_score'\n",
        "\n",
        "for dataset in progress_bar(DATASETS_SEMANTIC_SEG):\n",
        "  revised = '' if  dataset=='GFAP_in_HC' else '_revised'\n",
        "  mask_dir = 'masks_STAPLE'+revised\n",
        "  path = DATA_PATH/'data'/dataset/SUBDIR\n",
        "  gt_path = path/mask_dir\n",
        "  gt_masks_paths = [f for f in gt_path.iterdir()]\n",
        "\n",
        "  df_exp = pd.read_csv(path/f'STAPLE_vs_experts{revised}.csv')\n",
        "  df_exp['idx'] = df_exp['file'].str.split('_').str[0]\n",
        "  df_exp = df_exp.groupby(['idx']).agg(expert_min=(metric, np.min),\n",
        "                                        expert_mean=(metric, np.mean),\n",
        "                                        expert_max=(metric, np.max))\n",
        "\n",
        "  for method in progress_bar(METHODS_SEMANTIC_SEG, leave=False):\n",
        "    method_path = DATA_PATH/'results'/'semantic_segmentation'/dataset/method\n",
        "    results_method = []\n",
        "    \n",
        "    for repetition in range(1,4):\n",
        "      repetition_name = repetition_mapper(repetition, method, dataset)\n",
        "      pred_path = method_path/repetition_name\n",
        "\n",
        "      for f in gt_masks_paths:\n",
        "        idx = f.stem.split('_')[0]\n",
        "        msk = imageio.imread(f)//255\n",
        "        pred = imageio.imread(pred_path/f'{idx}.png')//255\n",
        "\n",
        "        # Calculate dice score\n",
        "        ds = dice_score(msk, pred)\n",
        "\n",
        "        tmp = pd.Series({\n",
        "          'dataset': dataset,\n",
        "          'method': method,\n",
        "          'repetition': str(repetition),\n",
        "          'repetition_name': repetition_name,\n",
        "          'idx': idx,\n",
        "           metric: ds,\n",
        "          'uncertainty_score': None\n",
        "          })   \n",
        "\n",
        "        if method=='deepflash2' and repetition==1:\n",
        "            # Load uncertainty scores\n",
        "            df_unc = pd.read_csv(method_path/f'1_uncertainty_scores.csv')\n",
        "            df_unc['idx']=df_unc.file.str[:-4] \n",
        "            tmp['uncertainty_score'] = df_unc.loc[df_unc.idx==idx]['uncertainty_score'].values[0]\n",
        "        \n",
        "        results_method.append(tmp)\n",
        "\n",
        "    # Relate to expert performance\n",
        "    df_method = pd.DataFrame(results_method)\n",
        "    df_method = df_method.set_index(['idx']).join(df_exp).reset_index()\n",
        "    results_semantic.append(df_method)\n",
        "\n",
        "df_semantic = pd.concat(results_semantic)\n",
        "df_semantic = expert_comparison(df_semantic, metric)\n",
        "df_semantic.to_csv(OUTPUT_PATH/'semantic_segmentation_results.csv', index=False)\n",
        "df_semantic.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3fUoaKJMzoC"
      },
      "source": [
        "# Instance segmentation and detection\n",
        "results_instance = []\n",
        "results_instance_agg = []\n",
        "metric = 'mean_average_precision'\n",
        "\n",
        "for dataset in progress_bar(DATASETS_INSTANCE_SEG):\n",
        "  revised = '_revised'\n",
        "  mask_dir = 'masks_STAPLE'+revised\n",
        "  path = DATA_PATH/'data'/dataset/SUBDIR\n",
        "  gt_path = path/mask_dir\n",
        "  gt_masks_paths = [f for f in gt_path.iterdir()]\n",
        "\n",
        "  df_exp = pd.read_csv(path/f'STAPLE_vs_experts{revised}.csv')\n",
        "  df_exp['idx'] = df_exp['file'].str.split('_').str[0]\n",
        "  df_exp = df_exp.groupby(['idx']).agg(expert_min=(metric, np.min),\n",
        "                                        expert_mean=(metric, np.mean),\n",
        "                                        expert_max=(metric, np.max))\n",
        "\n",
        "  for method in progress_bar(METHODS_INSTANCE_SEG, leave=False):\n",
        "    method_path = DATA_PATH/'results'/'instance_segmentation'/dataset/method\n",
        "    results_method_agg = []\n",
        "    \n",
        "    for repetition in range(1,4):\n",
        "      repetition_name = repetition_mapper(repetition, method, dataset)\n",
        "      pred_path = method_path/repetition_name\n",
        "\n",
        "      for f in gt_masks_paths:\n",
        "        idx = f.stem.split('_')[0]\n",
        "\n",
        "        # Load and clean gt mask\n",
        "        msk = imageio.imread(f)//255\n",
        "        _, label_msk = cv2.connectedComponents(msk.astype('uint8'), connectivity=4)\n",
        "        label_msk = clean_labels(label_msk, min_pixel=min_pixel_dict[dataset])\n",
        "\n",
        "        # Load and clean prediction\n",
        "        label_pred = tifffile.imread(pred_path/f'{idx}.tif')\n",
        "        label_pred = clean_labels(label_pred, min_pixel=min_pixel_dict[dataset])\n",
        "\n",
        "        # Calculate instance segmentation metrics\n",
        "        ap, tp, fp, fn = get_instance_segmentation_metrics(label_msk,\n",
        "                                                           label_pred, \n",
        "                                                           is_binary=False, \n",
        "                                                           thresholds=thresholds,\n",
        "                                                           )\n",
        "        # Detailed results\n",
        "        tmp = pd.DataFrame({\n",
        "          'dataset': dataset,\n",
        "          'method': method,\n",
        "          'repetition': str(repetition),\n",
        "          'repetition_name': repetition_name,\n",
        "          'idx': idx,\n",
        "          'threshold':thresholds,\n",
        "          'average_precision':ap\n",
        "          })   \n",
        "        results_instance.append(tmp)\n",
        "\n",
        "        # Aggregated results\n",
        "        tmp_agg = pd.Series({\n",
        "          'dataset': dataset,\n",
        "          'method': method,\n",
        "          'repetition': str(repetition),\n",
        "          'repetition_name': repetition_name,\n",
        "          'idx': idx,\n",
        "           metric: ap.mean(),\n",
        "          'average_precision_at_iou_50':ap[0]\n",
        "          })   \n",
        "        \n",
        "        results_method_agg.append(tmp_agg)\n",
        "\n",
        "    # Relate to expert performance\n",
        "    df_method = pd.DataFrame(results_method_agg)\n",
        "    df_method = df_method.set_index(['idx']).join(df_exp).reset_index()\n",
        "    results_instance_agg.append(df_method)\n",
        "\n",
        "df_instance = pd.concat(results_instance)\n",
        "df_instance.to_csv(OUTPUT_PATH/'instance_segmentation_results.csv', index=False)\n",
        "display(df_instance.tail())\n",
        "\n",
        "# Concat and save aggregated results\n",
        "df_instance_agg = pd.concat(results_instance_agg)\n",
        "df_instance_agg = expert_comparison(df_instance_agg, metric)\n",
        "df_instance_agg.to_csv(OUTPUT_PATH/'instance_segmentation_results_agg.csv', index=False)\n",
        "df_instance_agg.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UiXhpEjjsDmc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}